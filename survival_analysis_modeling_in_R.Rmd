---
title: "Survival Analysis Modeling in R"
output: html_notebook
author: "Clay Ford"
editor_options: 
  chunk_output_type: inline
---

This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
plot(cars)
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x" or simply run corrected code. 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## CODE ALONG 0

Enter a new code chunk and run the code `rnorm(10)` (sample 10 random values from a standard normal distribution.)


## Introduction

Survival analysis is a collection of procedures for analyzing the _time until an event occurs_. The methods were mostly developed in the medical field and concerned time until death, hence the name "survival analysis". Also known as _duration analysis_, _reliability analysis_, or _time-to-event analysis_. In this workshop we'll use the term "survival" generically.

What makes survival analysis challenging is that rarely do all subjects experience the event of interest. When a subject does not experience the event, they are _censored_.  The most common type of censoring is _right censoring_. Imagine time flowing as a straight line left to right. When subjects enter the study their timeline begins on the left. When the event happens, the subject's timeline ends on the right (x). If the event is not observed before the study ends (or the subject leaves the study), the subject's timeline is right censored (o).

subject 1 ----------------------x
subject 2 ----------------------------o

A study that is analyzed using survival analysis typically has an _observation period_ during which subjects are enrolled and observed, or it concerns a window of time where objects are observed. When a subject or object is censored, that means they didn't experience the event _during_ the observation period. It does not mean they will never experience the event. A key assumption of survival analysis is that the censoring is _uninformative_ and not related to when the subject will experience the event. To try and enforce this, many studies will specify a hard end date to a study.

In this workshop we want to get you up and running with _modeling survival data_. This is where we examine the relationship between survival time and predictors of interest. This is similar to traditional regression modeling, but is made complicated due to censoring.

## Load packages and data

We're going to use the following packages in this workshop. 

```{r}
library(survival)
library(survminer)
```

Let's load some data. Two groups of leukemia patients. Patients in remission were randomly assigned to maintenance therapy with either "6-MP" (a new treatment) or placebo. 21 on treatment, 21 on placebo. (Freireich et al, Blood (1963) 21 (6): 699–716.) The event of interest is "coming out of remission." Going forward when we talk about "survival" or "failure", we mean the event "coming out of remission".

The data is available on GitHub as an rds file. This is the file type of a stored R object. Below we enter the URL to the rds file as a character string. We then open the connection to the URL using the `url()` function. Finally we load the rds file using the the `readRDS()` function.

```{r}
URL <- "https://github.com/clayford/survival_analysis_modeling/raw/main/data/leuk.rds"
d <- readRDS(url(URL))
```

Variables:  
- survt (time in weeks; time until failure or censoring)
- status (0 = censored, 1 = failure)
- sex (0 = female, 1 = male)
- logwbc (log transformed white blood cell count)
- rx (treatment or placebo)

```{r}
head(d, n = 8)
```

It's important to note that binary "status" doesn't mean "success/failure" like it does in data analyzed with binary logistic regression. It indicates whether we observed the event of interest _during the observation period_.

Some questions of interest: 

- Does the 6-MP treatment increase time until "failure"? 
- Do we need to control for log WBC? 
- Is the treatment effect the same for males and females?

## Descriptive methods for survival data

Let's look at censoring by treatment group. All subjects come out of remission in the placebo group (21). 12 subjects were censored in the trt group. That is, they did not experience failure during the study.

```{r}
xtabs(~ rx + status, data = d)
```

We could also look at average survival times (ie, time observed until the event or censoring). Subjects on "trt" appeared to survive longer. But because there are several censored subjects in the trt group, the true survival time is even larger.

```{r}
aggregate(survt ~ rx, data = d, mean)
```

An alternative statistic is the _hazard rate_, defined by total number of failures divided by the sum of survival times.

```{r}
# failures (ie, events) per rx group
f <- xtabs( ~ rx, data = d, subset = status == 1)
# total survival time by rx group
total <- tapply(d$survt, d$rx, sum)
# hazard rate by rx group
hr <- f/total
hr
```

Higher hazard rate means lower probability of survival. A ratio of hazards is called the _hazard ratio_.

```{r}
hr[2]/hr[1]
```

It appears hazard of remission is about 4 times higher in the placebo group. However these are just overall comparisons. We prefer to describe the data _over time_.

The _Kaplan-Meier (KM) curve_ is the workhorse function for visualizing and describing survival data over time. This method was published by Edward Kaplan and Paul Meier in 1958. We can create a KM curve using the `survfit()` function from the {survival} package. To do this, we need to define survival time data using the `Surv()` function. The first argument is survival time, the second argument is the censoring indicator. Values with "+" appended are censored values. For example, the first subject went 35-plus weeks without going out of remission.

```{r}
Surv(d$survt, d$status)
```

The `survfit()` function has `formula` and `data` arguments, so we can directly use the variable names in the `Surv()` function. The `~ 1` says we want to fit a single survival curve for the entire data set.

```{r}
fit <- survfit(Surv(survt, status) ~ 1, data = d)
fit
```

n: number of subjects  
events: number of subjects who experienced the event  
median: median survival time (12 weeks)  
0.95LCL, 0.95UCL: lower/upper 95% CI on median survival time [8,22]

Using `plot()` we can create a KM survival curve with 95% confidence bands (dashed lines). The y-axis is probability of "survival". Each "step" down represents 1 or more subjects experiencing the event. Adding `mark.time = TRUE` indicates censored observations. The `pch` argument lets you choose the censoring symbol. (see `?points` for code definitions.) There appears to be about a 0.8 probability of not experiencing a remission to week 5.

```{r}
plot(fit, mark.time = TRUE, pch = 8)
```

See the section "How to calculate Kaplan-Meier estimates" in the appendix of this document for details on how the curves are calculated.

Calling summary on the KM objects returns survival probabilities (probability of failure, or coming out of remission) for each observed point in time.

```{r}
summary(fit)
```

We can specify specific times in `summary()` using the `times` argument. Below we look at weeks 5, 10, 15, and 20

```{r}
summary(fit, times = seq(5,20,5))
```

We're probably more interested in time-until-failure by treatment group (rx). For this we specify `Surv(survt, status) ~ rx`. Now we see different estimated median survival times _by rx group_.

```{r}
fit2 <- survfit(Surv(survt, status) ~ rx, data = d)
fit2
```

Estimated median time-to-failure for subjects in "trt" group is 23 weeks versus 8 weeks for the "placebo" group. 95% confidence intervals on the median estimates are provided. Notice the upper limit is NA for the "trt" group. This is because less than half the subjects in this group came out of remission.

This fit generates two KM curves. There is a `plot()` method but it looks pretty basic without some extra work. At a minimum we need to use the `lty` (line type) argument to distinguish between groups

```{r}
plot(fit2, lty = 1:2, mark.time = TRUE, pch = 8)
```

The `ggsurvplot()` function from the {survminer} package looks nicer and includes "+" symbols to indicate censored subjects and a legend to identify groups.

```{r}
ggsurvplot(fit2)
```

Setting `risk.table = TRUE` adds a table below the plot showing numbers of subjects at risk at various times in the observation period. The `tables.height = 0.3` argument specifies the height of the table. I arrived at this value by trial and error.

```{r}
ggsurvplot(fit2, risk.table = TRUE, tables.height = 0.3)
```

Finally we might be interested in testing the null hypothesis that both survival curves take the same trajectory. To answer this we can use the _log-rank test_, which is implemented in the `survdiff()` function. A small p-value provides evidence against the null hypothesis. Below we have good evidence the KM curves differ by rx group.

```{r}
survdiff(Surv(survt, status) ~ rx, data = d)
```

We usually want to analyze time-to-event _adjusting for additional variables_, such as age, lab values, patient history, etc. For this we move on to regression modeling.

## CODE ALONG 1

For our code alongs we'll work with data from the Worcester Heart Attack Study (WHAS). The main goal of this study was to describe factors associated with survival time following hospital admission for acute myocardial infarction, aka heart attack. The time variable is "time" (in years) and the censoring variable is "fstat".

```{r}
URL2 <- "https://github.com/clayford/survival_analysis_modeling/raw/main/data/WHAS.rds"
whas <- readRDS(url(URL2))
```


(1) Create a KM curve for time versus "gender" (0 = male, 1 = female)


(2) What are the estimated survival probabilities for years 1 - 5?


(3) Run a log-rank test on the KM curves for gender



## Regression models for survival data

Regression models allow us to study the effect of multiple variables on the survival experience. For modeling survival time it is convenient to model the _hazard of the event_ instead of the raw time-until-event or probability of event. This means estimating a _hazard function_.

It turns out that once we estimate a hazard function, we can also estimate a survival function, which means we can use our model to produce _adjusted survival curves_ similar to the KM curves we created to describe the data. 

The standard regression model for survival data is the _Cox Proportional Hazards Model_, named for David Cox who published the method in 1972. The Cox Proportional Hazards (Cox PH) Model makes _no assumption_ about the distribution of the time-to-event. It is usually a safe and robust model. Even if there is a better-fitting "correct" parametric model (i.e., a model that makes assumptions about the distribution of survival time), the Cox PH model will often closely approximate the results.

The Cox PH model stated mathematically is as follows:

$$h(t,X) = h_{0}(t)e^{X\beta}$$

Stated in words, this says, "the hazard is a product of the baseline hazard (h0) and an exponentiated weighted sum of the predictors, where the weights are the betas (ie, model coefficients)." The Cox PH model _only_ estimates the model coefficients, it does not directly estimate the baseline hazard. Thus the Cox PH model estimates the _proportional change_ from some baseline hazard function. Only the relative hazard is modeled.

The {survival} package provides the `coxph()` function for fitting Cox PH models. We use the function as we would `lm()` or `glm()`, but with the left-hand side requiring the use of the `Surv()` function.

Below we model time-to-remission as a function of rx and save as `m1` and then use `summary()` to request the model summary.

```{r}
m1 <- coxph(Surv(survt, status) ~ rx, data = d)
summary(m1)
```

The coefficient we're usually most interested in is the one labeled "exp(coef)". This is the _hazard ratio_. The interpretation above is that the hazard of failure for those on "placebo" is about 4.8 times higher than the hazard of failure for those on "treatment".

- "se(coef)" is the _standard error_ of the coefficient.
- "z" is the _test statistic_ of the coefficient. z = coef/se(coef)
- "Pr(>|z|)" is the _p-value_, or probability of getting a z value bigger than the z observed (in absolute value) if the coefficient is truly 0. A small p-value provides evidence against the null hypothesis of the coefficient being 0.

The next section shows a 95% confidence interval on the hazard ratio. The hazard of failure for those on placebo is plausibly anywhere from 2 to 10 times higher than the hazard of failure for the trt group.

The "exp(-coef)" is the hazard ratio in the opposite direction. The hazard of failure on "treatment" is about 80% lower than the hazard of failure on "placebo".

The statistic labeled "Concordance" reports the fraction of all pairs of subjects where the model correctly predicts the individual with the earlier event. Values in the 0.50 - 0.55 range suggest a flip of a coin would do as well as our model at correctly ordering pairs of subjects. The value of 0.69 (+/- 0.04) is promising but not excellent. 

The three tests at the end test the null that all model coefficients are simultaneously equal to 0. These are usually all in agreement. When these disagree, go with the Likelihood Ratio Test (Hosmer et al, p. 79)

There is no R-squared reported. According to Hosmer et al, "there is not a single, simple, useful, easy to calculate and easy to interpret [R-squared] measure for a proportional hazards regression model." (p. 194)

We can use our Cox PH model to create _covariate_adjusted survival curves_. To do so, we need to first use the `survfit()` function on the model object. Notice we need to set the covariate values to create the curves. Below we specify we want a curve for each level of rx.

```{r}
sfit1 <- survfit(m1, newdata = data.frame(rx = c("trt", "placebo")))
```

Now we're ready to make the plot. We can use `ggsurvplot()`, but notice we get warnings about deprecated functions. It appears the {survminer} package is not in active development or maintenance at the time of this writing (Oct 2023). However the package still seems to work and averages about 66,000 downloads per month.

```{r}
# set conf.int = FALSE to suppress CI ribbons
ggsurvplot(sfit1, data = d)
```

Notice this plot assumes proportional hazards and uses all the data. Compare to the Kaplan-Meier curve below, which only uses the data in each treatment group and does not assume proportional hazards. Recall the KM curve is non-pararmetric and descriptive.

```{r}
# KM plot
ggsurvplot(fit2)
```

There's a base R `plot()` method for survfit objects. Here's one way to make adjusted survival curves. (Run entire code chunk at once.)

```{r}
plot(sfit1, lty = 1:2, xlab = "Weeks", ylab = "Survival probability")
legend("bottomleft", lty = 1:2, legend = c("trt", "placebo"))
```

If we would like to indicate censored times in the base R plot, we can add `mark.time = TRUE` to the `plot()` function.

```{r}
plot(sfit1, col = c("orange3", "royalblue2"), 
     xlab = "Weeks", ylab = "Survival probability", mark.time = TRUE)
legend("bottomleft", lty = 1, col = c("orange3","royalblue2"), 
       legend = c("trt", "placebo"), bty = "n")
```


## CODE ALONG 2

(1) Fit a Cox PH model for time on gender using the WHAS data. Is this a good model for understanding survival time after admission to hospital for a heart attack?


(2) How do we interpret the coefficient for gender?



(3) Plot adjusted survival curves for the Cox PH model for each gender level.




## The proportional hazards assumption

The Cox PH model assumes that the _hazard ratio is constant over time_. The model for the leukemia data above implies the hazard of failure on placebo is about 4.8 times higher than the hazard of failure on treatment, no matter how long we observe the subjects. Notice the summary output above makes no reference to time.

Fortunately the {survival} package makes it pretty easy to assess these assumptions using the `cox.zph()` function. Use it on your model object. The null hypothesis is the hazard ratios are independent of time. A small p-value is evidence against the null hypothesis. The proportional hazard assumption is probably safe if the p-value is higher than, say, 0.05.

```{r}
cox.zph(m1)
```

Each variable is tested. The GLOBAL test is that all variables _simultaneously_ satisfy the proportional hazards assumption. In this case they're the same since we only have one predictor. Set `global=FALSE` to suppress the global test.

A visual check of the proportional hazards assumption is available using the `plot()` method. A smooth trend line is plotted through residuals versus time. (Residuals are complicated for survival models. We talk more about them below.) A fairly straight line indicates the proportional hazards assumption is likely satisfied. The dashed lines are +/- two standard error confidence bands for the smooth line. We would like to be able to draw a straight line through the middle of the confidence bands. This implies the model predictions, which are relative hazards, are staying consistent over time. 

```{r}
plot(cox.zph(m1))
```

The `ggcoxzph()` function from the {survminer} package makes a slightly fancier plot that includes the result of the hypothesis test.


```{r}
ggcoxzph(cox.zph(m1))
```


If the proportional hazards assumption appears to be violated, we may need to consider fitting a _stratified Cox PH model_ (among other options). We discuss this later in the workshop.

## CODE ALONG 3

(1) Check the proportional hazards assumption of the model we fit in CODE ALONG 2 using a test.


(2) Visually check the proportional hazards assumption of the model we fit in CODE ALONG 2.


## Cox PH model with multiple predictors

In our previous model we only looked at the effect of "rx" on time to failure. However we may also want to adjust for white blood cell count ("logwbc"). This is a known prognostic indicator of coming out of remission for leukemia patients. Indeed we can see an association between "survt" (time-to-failure) and "logwbc", for those subjects who experienced the event (status = 1).

```{r}
plot(survt ~ logwbc, data = d, subset = status == 1)
```

Below we add the variable "logwbc" (log-transformed white blood cell count) to the model. This allows us to estimate placebo and treatment effects on patients with similar white blood cell counts. 

```{r}
m2 <- coxph(Surv(survt, status) ~ rx + logwbc, data = d)
summary(m2)
```

Adjusted for white blood cell count, it appears the hazard of failure for subjects on placebo is about 4 times higher than the hazard of failure for subjects on treatment. Likewise, adjusted for rx, the hazard of failure increases by about a factor of 5 for each one unit increase in logwbc. (In this case, that's probably not an interpretation we're interested in. We simply want to adjust for subjects' white blood cell count.)

Adjusting for logwbc reduces the effect of rx about 12%. This is a sign that logwbc may be _confounded_ with rx, and that we should include it in our model.

```{r}
coef(m2)[1]/coef(m1)[1]
```

The confidence interval on the rx hazard ratio is also tighter (more precise) in the model with logwbc.

Model 1 without logwbc
CI width (upper bound - lower bound):    
10.81 - 2.147 = 8.663

Model 2 with logwbc
CI width (upper bound - lower bound):  
9.195 - 1.739 = 7.456

We can formally compare models using the `anova()` function. The null hypothesis of this test is that the models are equally adequate. The test below suggests we should keep logwbc in our model.

```{r}
anova(m1, m2)
```

We should assess the proportional hazards assumption again. Recall this assumption applies to all predictors in the model.

```{r}
cox.zph(m2)
```


```{r}
plot(cox.zph(m2))
```

Or using `ggcoxzph()`

```{r}
ggcoxzph(cox.zph(m2))
```


Once again we can visualize our Cox PH model creating survival curves for _particular groups of interest_. As before we specify the groups in a new data frame. Below we specify two groups: a treated group with median logwbc and placebo group with median logwbc. We then call `ggsurvplot()` on the survfit object.

```{r}
nd <- data.frame(rx = c("placebo","trt"), logwbc = median(d$logwbc))
sfit2 <- survfit(m2, newdata = nd)
ggsurvplot(sfit2, data = nd, conf.int = FALSE)
```

Another example: "trt" groups with different logwbc levels. Notice the `ggsurvplot()` offers a `legend.labs` argument to change the legend labels. We set the confident interval off for demo purposes, but setting it to TRUE reveals the enormous uncertainty in these estimates.

```{r}
nd <- data.frame(rx = "trt", logwbc = c(2.5,3,3.5))
sfit3 <- survfit(m2, newdata = nd)
ggsurvplot(sfit3, data = nd, conf.int = FALSE,
           legend.labs = paste(nd$rx, nd$logwbc))
```


## CODE ALONG 4

(1) Fit a Cox PH model to the WHAS data using gender, age, hr (initial heart rate), and bmi.

(2) Assess the proportional hazards assumption of the model.


(3) What's the interpretation of the age coefficient?


(4) Visualize estimated survival proportions for males (gender = 0) ages 50,60, and 70 with median hr and median bmi.



## The Stratified Cox Model

Let's add the sex variable to our model. It is binary (1 = male, 0 = female). Perhaps our analysis plan specified that we would adjust for sex. This means retaining the sex variable regardless of significance. Below the coefficient is slightly positive, suggesting a higher hazard of failure for males. But the standard error is large and the z statistic is small. We're not sure what effect sex has on time to failure. Nevertheless we opt to keep it in the model.

```{r}
m3 <- coxph(Surv(survt, status) ~ rx + logwbc + sex, data = d)
summary(m3)
```

Now let's assess the proportional hazards assumption for all variables. The null is all variables satisfy the assumption. It appears we have evidence against the null for the sex variable.

```{r}
cox.zph(m3, global = FALSE)
```

Likewise the plot shows a curvy line. We use `var = "sex"` to see just the plot for sex. 

```{r}
plot(cox.zph(m3), var = "sex")
```

One way to address a violation of the proportional hazards assumption is with a _stratified Cox model_. The general idea is to group the data according to the strata of a variable that violates the assumption, fit a Cox PH model to each strata, and combine the results into a single model. The coefficients of the remaining variables are _assumed to be constant across strata_. We might think of them as "pooled" estimates. 

A drawback of this approach is the inability to examine the effects of the stratifying variable. On the other hand, we may view it as a unique feature that allows us to _adjust for variables that are not modeled_. Stratification is most natural when a covariate takes on only a few distinct values (like sex), and when the effect of the stratifying variable is not of direct interest.

We can implement a stratified Cox model by wrapping the variable to stratify on in the `strata()` function. Notice that sex is no longer reported in the model. Since we stratified on it, we do not estimate its effect. Also, the effects of rx and logwbc are assumed to be the same for males and females. This is called the _no-interaction assumption_.

```{r}
m4 <- coxph(Surv(survt, status) ~ rx + logwbc + strata(sex), data = d)
summary(m4)
```

We can test the no-interaction assumption by fitting a new model that allows the stratification to interact with the other variables, and then comparing this more complex model to the previous no-interaction model using a likelihood ratio test via `anova()`. The null is no difference in the models.

```{r}
m5 <- coxph(Surv(survt, status) ~ (logwbc + rx) * strata(sex), 
            data = d)
summary(m5)
```

Compare the interaction model with the no-interaction model.

```{r}
anova(m4, m5)
```

It appears the simpler no-interaction model is sufficient.

Finally, we can re-check the proportional hazards assumption of the stratified model.

```{r}
cox.zph(m4)
```

The stratified Cox PH model is commonly used in practice to adjust for multiple sites in large clinical trials.

Again we can plot adjusted survival curves for the variables we modeled. Below we create four adjusted survival curves with logwbc held at the median value of 2.8. 

1. Female, trt
2. Female, placebo
3. Male, trt
4. Male, placebo

Notice we create the first plot for females by using the `suvfit` indexing method. Rows 1 and 2 of the new data are for females, so we select just those rows using `sfit4[1:2]`.

```{r}
nd <- expand.grid(rx = c("trt", "placebo"), sex = 0:1)
nd$logwbc <- median(d$logwbc)
sfit4 <- survfit(m4, newdata = nd)
ggsurvplot(sfit4[1:2], data = d, legend.labs = c("trt", "placebo"), 
           title = "Female strata")
```

And we can create a plot for males using `sfit4[3:4]`

```{r}
ggsurvplot(sfit4[3:4], data = d, legend.labs = c("trt", "placebo"), 
           title = "Male strata")
```



## CODE ALONG 5

(1) Fit a Cox PH model to the WHAS data using gender, age, hr (initial heart rate, and bmi, stratified on year (Cohort Year).

```{r}
whas_m3 <- coxph(Surv(time, fstat) ~ gender + age + hr + bmi + strata(year), 
                 data = whas)
```

(2) Test the no-interaction assumption by fitting a new model that allows the stratification on year to interact with the other variables.

```{r}
whas_m4 <- coxph(Surv(time, fstat) ~ (gender + age + hr + bmi) * strata(year), 
            data = whas)
anova(whas_m3, whas_m4)
```

(3) Do we need to stratify on year?

```{r}
whas_m5 <- coxph(Surv(time, fstat) ~ gender + age + hr + bmi + year, 
                 data = whas)
cox.zph(whas_m5)
```


## Regression diagnostics

When it comes to survival analysis, it's important to check if any subjects have high _leverage_ or _influence_. 

- Leverage: unusually large or small variables relative to rest of data
- Influence: removal would significantly change the model coefficients

High leverage itself is not necessarily a concern, however a subject with leverage may influence the estimation of model coefficients.

_Residuals_ are central to model diagnostics. Residuals are the difference between what we observe and what the model predicts. However, "there is no obvious analog to the usual 'observed minus predicted' residual used with other regression methods." (Hosmer et al, p. 170) Recall that we deal with censored observations (ie, no observed value). This complicates matters. Instead, different types of residuals have been developed based on martingale theory. The math behind these residuals is difficult and we will take it on faith that it works.

To assess leverage, we can use _score residuals_. Each subject in a Cox PH model will have a _separate score residual for each variable_ in the model. The larger a score residual for a particular variable (in absolute value), the larger the "leverage".

We can extract score residuals using the `residual()` function with `type = score`. Below we request score residuals for model m4. Notice there is a _separate residual_ for each variable in the model.

```{r}
r <- residuals(m4, type = "score")
head(r)
```

For continuous variables, we can plot the score residual versus the variable. Below we check the logwbc residuals. We appear to have four subjects with unusually large residuals (in absolute value).

```{r}
plot(r[,"logwbc"] ~ d$logwbc)
```

For categorical variables, we can create boxplots of the score residuals versus the variable. Again we have a few outlying subjects we should investigate.

```{r}
boxplot(r[,"rxplacebo"] ~ d$rx)
```

For logwbc, we could check which subjects have a score residual less than -0.5

```{r}
i <- which(r[,"logwbc"] < -1) # select rows
i
```

Likewise we can check the rx score residuals.

```{r}
j <- which(r[,"rxplacebo"] < -0.4)
j
```

Subject 19 appears in both results. We can inspect the records for these subjects as follows. The `union()` function basically prevents subject 19 from appearing twice below.

```{r}
d[union(i,j),]
```

Subject 19 appears to stand out by being in the treatment group but experiencing "failure" at only 6 weeks. 

To assess influence, we can use _scaled score residuals_, also known as "dfbetas". Each subject in a Cox PH model will have a _separate dfbeta for each variable_ in the model. The larger a dfbeta for a particular variable (in absolute value), the larger the "influence".

We can extract score residuals using the `residual()` function with `type = dfbeta`. Below we request dfbetas for model m4. Again, notice there is a separate residual for each variable in the model. 

```{r}
r2 <- residuals(m4, type = "dfbeta")
head(r2)
```

These residuals are simply the score residuals multiplied by the model covariance matrix, which can be obtained with the function `vcov()`.

```{r}
# Use %*% for matrix multiplication
head(residuals(m4, type = "score") %*% vcov(m4)) 
```

Again we can use scatter plots and box plots to investigate influence.

```{r}
plot(r2[,2] ~ d$logwbc)
```

Four subjects seem to stand out:

```{r}
which(r2[,2] < -0.1)
```


```{r}
boxplot(r2[,2] ~ d$rx)
```

Three subjects appear to have outlying dfbeta values.

```{r}
which(r2[,1] < -0.1)
```

Once again subject 19 appears in both residual sets as having a large residual relative to the rest of the data.

To find out how variables are influencing a model, we need to drop them and refit the model. With such a small data set, we would want to think carefully before permanently dropping subjects. However, let's say we want to see how much subject 19 is influencing the model. We can refit the model without subject 19 and calculate the change in the rx coefficient.

```{r}
m4a <- coxph(Surv(survt, status) ~ rx + logwbc + strata(sex), 
             data = d[-19,])
coef(m4a)["rxplacebo"]/coef(m4)["rxplacebo"]
```

With subject 19 removed from the model, the coefficient estimate for rx increases by almost 15%, which makes the treatment look even better. This doesn't mean we should drop the subject. It just helps us understand the impact a single subject has on our model. The decision to drop subjects should be carefully considered and not made based on just residual values.

The {survminer} package offers the `ggcoxdiagnostics()` function to create similar plots, but is currently (as of Sept 2023) throwing warnings about deprecated tidyverse functions. It also creates the same type of plot regardless whether the predictor is numeric or categorical.

```{r}
ggcoxdiagnostics(m4, type = "score")
```


## CODE ALONG 6

In the previous code along we fit the following model:

```{r}
whas_m3 <- coxph(Surv(time, fstat) ~ gender + age + hr + bmi + year, 
                 data = whas)
summary(whas_m3)
```

(1) Check the bmi score residuals for subjects with high leverage.



(2) Check the bmi dfbetas for subjects with high influence.


(3) Investigate any subjects that are turned up in these assessments and see how their removal from the model changes the results.



## The Extended Cox Model

So far we have not incorporated time into our models. Time comes into play in two different ways:

1. _Time-dependent coefficients_. We may have model coefficients that vary with time. In other words, they may _interact with time_ and change. For example, the effect of a treatment on the hazard may decrease over time. Or the effect of age on the hazard may increase over time.
2. _Time-dependent variables_. We may have variables that _change over time_. In other words, we may have multiple measures on variables. For example, we may have weekly measurements of white blood cell counts, or we may have a time-dependent intervention that happens during the observation period, such as a transplant.

In either case we need to _extend_ the Cox PH model to incorporate time. Recall the standard Cox PH model. Notice time is not part of the linear predictor (XB). Time is only part of the baseline hazard which is not estimated.

$$h(t,X) = h_{0}(t)e^{X\beta}$$

The _model for time-dependent coefficients_ allows coefficients to depend on time:

$$h(t,X) = h_{0}(t)e^{X\beta(t)}$$

The _model for time-dependent variables_ allows variables to depend on time:

$$h(t,X) = h_{0}(t)e^{X(t)\beta}$$

Let's look at one example of each.

### Time-dependent coefficients

Recall the proportional hazards assumption: the hazard ratio is _constant over time_. When this assumption is violated for a _continuous variable_, we may wish to incorporate a time-dependent coefficient. (For a categorical variable, we would probably pursue a stratified Cox PH model.)

To illustrate we'll use the veteran data set that comes with the {survival} package, which has data on a randomized trial of two treatment regimens for lung cancer. Below we model survival time as a function of treatment (1=standard, 2=test) and Karnofsky performance score (0 - 100), which is a scale that measures a patient's ability to carry out activities of daily living. Higher score is better. The data has information on 137 subjects.

```{r}
nrow(veteran)
```

Notice the test for proportional hazards is soundly rejected for the "karno" variable.

```{r}
tdm1 <- coxph(Surv(time, status) ~ trt + karno, data = veteran)
cox.zph(tdm1)
```

Notice how wiggly the line is in the associated plot. 

```{r}
plot(cox.zph(tdm1), var = "karno")
```

At first glance it seems we can simply add the time variable to the right-hand side of the model to model a time-dependent coefficient. But this would be a mistake. The `coxph()` function will issue a warning if you try this.

```{r}
tdmX <- coxph(Surv(time, status) ~ trt + karno + karno:time, data = veteran)
```

The correct way is to use a _time-transform_ function via the `tt` argument. The usual syntax is `function(x, t, ...) x * log(t)` where `x` is the argument for the predictor, `t` is the argument for `time`, and `...` is the "dots" argument used to pass through other arguments from other functions. 

```{r}
tdm2 <- coxph(Surv(time, status) ~ trt + karno + tt(karno), 
              data = veteran,
              tt = function(x, t, ...) x * log(t))
summary(tdm2)
```

The time dependent coefficient is estimated as -0.083146 + 0.013236*log(t). It appears to be significant based on the z and p values.

We cannot formally compare the model with time-dependent coefficient to the first model because they are _fit to different size data sets_. Notice the error message when we try using `anova()`:

```{r}
anova(tdm1, tdm2)
```

If we set `x = TRUE` when fitting the models we can save the predictor data used to fit the model and compare the sizes. Notice the second model has 5955 rows of data versus only 137 for the first.

```{r}
tdm1 <- coxph(Surv(time, status) ~ trt + karno, data = veteran, x = TRUE)
tdm2 <- coxph(Surv(time, status) ~ trt + karno + tt(karno), data = veteran,
              tt = function(x, t, ...) x * log(t), x = TRUE)
c("tdm1" = nrow(tdm1$x), "tdm2" = nrow(tdm2$x))
```

Using a time-transform function _expands the data_ to create multiple observations per subject over time. The number of observations per subject depends on the time value for the subject and how they rank relative to the rest of the subjects. If you have a small time relative to everyone else, you have few records. If you have a large time, you have many records. In this case, everyone's "karno" score needs to be evaluated at every time point until they experience the event or are censored. 

Below we show the expansion for the first four subjects. Subjects 85 and 77 were the first to experience the event. Subject 95 was the second, so they get two records. Subject 53 was the third, so they get three records, and so on. (The subject "numbers" are row numbers in the original data set.)

```{r}
X <- cbind(tdm2$x, tdm2$y, subj = trunc(as.numeric(rownames(tdm2$x)))) |>
  as.data.frame()
tmp <- subset(X, subj %in% c(85,77,95,53))
tmp[order(tmp$subj, tmp$time),]
```

If we want, we can expand the data ourselves using the `survSplit()` function from the {survival} package. This function splits each record into multiple subrecords at each cut time. To match what the time-transform function did, we need to specify the _unique uncensored times_ as the cut times. When finished, we have a dataset with 5955 rows and extra a column called "tstart".

```{r}
dtimes <- unique(veteran$time[veteran$status == 1])
veteran_long <- survSplit(Surv(time, status) ~ ., 
                          data = veteran, 
                          cut = dtimes)
str(veteran_long)
```

Now we re-fit both models to the expanded data. Notice we need to _include the time interval_ in the `Surv()` function: `Surv(tstart, time, status)`. This time it's OK to put "time" on the right-hand side of the model. 

```{r}
tdm3 <- coxph(Surv(tstart, time, status) ~ trt + karno, 
              data = veteran_long)
tdm4 <- coxph(Surv(tstart, time, status) ~ trt + karno + karno:log(time), 
              data = veteran_long)
```

Verify these model results match the previous fits.

```{r}
list("n = 137" = coef(tdm1), "n = 5955" = coef(tdm3), 
     "n = 137" = coef(tdm2), "n = 5955" = coef(tdm4))

```


Now we can formally compare the models using the `anova()` function. It appears the model with the time-dependent coefficient is better.

```{r}
anova(tdm3, tdm4)
```


### Time-dependent variables

We sometimes have variables that change over time (age, weight, status, etc). To accommodate these variables we can no longer assume proportional hazards. 

Time-dependent variables are usually coded as _intervals of time_. This is sometimes called the _counting process method_. For example, lab values taken at day 0, 90, and 160:

```
    subject time1 time2 death  lab_value
1         1     0    90     0        0.7
2         1    90   160     0        1.9
3         1   160   220     1        1.3
```

We can read the first row as "over the interval from 0 to 90 the lab value for subject 1 was 0.7 and that this interval did not end in a death."

Note: we cannot interpolate time-dependent values. For example, we cannot suppose the lab value is 1.3 at day 45.

The classic example of a Cox PH model with a time-dependent variable is the Stanford Heart Transplant study. 103 patients were enrolled in a program to receive a heart transplant between 1967 and 1974. Of interest is survival time while waiting for a heart transplant and after receiving a transplant. This data is available with the {survival} package as `jasa1`. The time-dependent variable is transplant. Notice subject 4 was observed for 35 days, at day 35 received a transplant, and died at day 38. (age = age - 48; year = years after 1 Nov 1967; surgery = prior bypass surgery (1 = yes))

```{r}
data(heart, package="survival")
head(jasa1, n = 10)
```

To include a time-dependent variable in a Cox PH model, we need to _include the time interval_ in the `Surv()` function. Notice below we have `Surv(start, stop, event)`. Here we model the hazard of death as a function of whether the subject had a transplant, whether they had prior bypass surgery, and their age.

```{r}
tdm5 <- coxph(Surv(start, stop, event) ~ transplant + surgery + age,
             data= jasa1)
summary(tdm5)
```

According to this simplistic model, prior bypass surgery seems to lower the hazard of death when while waiting for/after receiving a heart transplant by about 54%.

Visualizing Cox PH models with time-dependent variables or coefficients is a bit tricky. See the "Using Time Dependent Covariates" vignette for some pointers. 

```{r}
vignette("timedep", package = "survival")
```


## Wrap-up

This was a short introduction to survival analysis. There is much we did not cover, such as:

- Right-censored survival times
- Parametric regression models
- Recurrent event models
- Frailty models (mixed-effect models for survival analysis)
- Competing risks models

Hopefully this workshop will give you the foundation to continue learning more about survival analysis.

## References

- Allison, P. (2010). _Survival Analysis Using SAS, 2nd Ed_. SAS Publishing.
- Austin, P. (2012) Generating survival times to simulate Cox proportional hazards models with time-varying covariates. Statistics in Medicine, 31(29):3946-58.
- Bender R., Augustin T., and Blettner M. (2005) Generating survival times to simulate Cox proportional hazards models. Statistics in Medicine, 24(11):1713–23.
- CRAN Task View: Survival Analysis, https://cran.r-project.org/web/views/Survival.html 
- Harrell, F. (2015). _Regression Modeling Strategies, 2nd Ed_. Springer. 
- Hosmer, D., Lemeshow, S., and May, S. (2008). _Applied Survival Analysis, 2nd Ed_. Wiley.
- Kleinbaum, D. and Klein, M. (2005). _Survival Analysis, a Self-Learning Text, 2nd Ed_. Springer.
- Moore, D. (2016) _Applied Survival Analysis Using R_. Springer.
- https://www.publichealth.columbia.edu/research/population-health-methods/time-event-data-analysis
- https://missingdatasolutions.rbind.io/2022/12/cox-baseline-hazard/
- Freireich, et al. (1963) The Effect of 6-Mercaptopurine on the Duration of Steroid-induced Remissions in Acute Leukemia. Blood (1963) 21 (6): 699–716. https://doi.org/10.1182/blood.V21.6.699.699


## Appendix

### Dealing with ties

When measuring time until an event, it's possible to have ties. For example, three subjects may experience an event at week 8. The `coxph()` function offers three options for handling ties in models when it comes to fitting a cox model:

1. Efron ("efron")
2. Breslow ("breslow")
3. Exact Partial Likelihood ("exact")

These can be specified by setting the `ties` argument. The default setting for the `coxph()` function is "efron". According to the coxph help page, "it is more accurate when dealing with tied death times, and is as efficient computationally." The "breslow" option is the original method for dealing with ties and is the default option in programs such as Stata and SAS. "The calculation of the exact partial likelihood is numerically intense." 

If you're trying to replicate the results of a study and getting slightly different coefficients, there's a good chance the data has ties and the study used a different method for handling ties. 

If there are no tied death times all the methods are equivalent. 

And finally, as the `coxph()` help pages says: "In practice the number of ties is usually small, in which case all the methods are statistically indistinguishable." In other words, you probably don't need to worry about ties.

### Simulating data for a Cox PH model

The Cox PH model stated mathematically is as follows:

$$h(t,X) = h_{0}(t)e^{X\beta}$$

Stated in words, this says, "the hazard is a product of the baseline hazard (h0) and an exponentiated weighted sum of the predictors, where the weights are the betas (ie, model coefficients)." The Cox PH model _only_ estimates the model coefficients, it does not estimate the baseline hazard.

We can use this model to simulate data for a Cox PH model. Below we simulate 800 observations using age and sex as predictors. Age is drawn from a Normal distribution with mean = 50. Sex is random selection of 'Male' and 'Female'. The "true" coefficients are 0.04 for age and 0.8 for sex='Male'. The baseline hazard is 0.05. Notice we center Age when we generate the hazard (h). That's because the `coxph()` function internally scales and centers data.

```{r}
n <- 800
set.seed(1)
age <- rnorm(n = n, mean = 50, sd = 10)
sex <- factor(sample(c('Male','Female'), n, replace = TRUE))
# baseline hazard
h0 <- 0.05
# simulate hazard using Cox PH model
h <- h0 * exp(0.04 * (age - 50) + .8 * (sex == 'Male'))
summary(h)
```

Notice the hazard ranges in value from about 0.01 to 0.51. We need to convert this to time-until-an-event. Bender et al show a method to generate survival time as follows:

$$T = -\frac{\text{log}(u)}{h} $$

where "u" is a random draw from a uniform distribution ranging from 0 to 1.

```{r}
set.seed(1)
time <- -log(runif(n))/h
summary(time)
```

Now we see time ranging from 0.003 to about 191.

Let's also generate a censoring indicator. Below we set about 15% of the observations to be censored. This time we use `runif(n)` as a preference rather than a requirement. About 15% of the values should be less than 0.15. Those values produce a 0 (censored). Otherwise they return a 1. Notice the censoring is not informative. In other words, it's not associated with any of the predictors or any other process. That's an assumption of the Cox PH model.

```{r}
set.seed(1)
status <- ifelse(runif(n) < 0.15, 0, 1)
mean(status == 0)
```

Now we "work backwards" to see how close we come to estimating the true coefficients. The 95% confidence interval for sex captures the true value but not the one for age.

```{r}
m <- coxph(Surv(time, status) ~ age + sex)
confint(m)
```

However the model summary reports both coefficients as being positive and different from 0.

```{r}
coef(summary(m))
```


It's worth noting that the baseline hazard value does not affect the coefficient estimates. Feel free to change the `h0` value above to different values and notice the coefficients do not change. This is because the Cox PH model does not estimate the baseline hazard. 

It's also worth pointing out that this model features _time-independent predictors_. To generate data using time-dependent predictors requires more work. See Austin (2012) for details. 

Being able to generate data using a CoxPH model can help in planning experiments and estimating power and sample sizes based on assumed effect sizes and assumed proportion censored. Using our model above, what is the power of our experiment to detect an effect for sex = 'Male' if we assume an effect size of 0.6, a sample size of 100, an age effect of 0.04, and censoring around 30%?

```{r}
study <- function(n, c){
  age <- rnorm(n = n, mean = 50, sd = 10)
  sex <- factor(sample(c('Male','Female'), n, replace = TRUE))
  h0 <- 0.05
  h <- h0 * exp(0.04 * (age - 50) + .6 * (sex == 'Male'))
  time <- -log(runif(n))/h
  status <- ifelse(runif(n) < c, 0, 1)
  m <- coxph(Surv(time, status) ~ age + sex)
  coef(summary(m))["sexMale","Pr(>|z|)"] < 0.05 # less than 0.05?
}
# run study 1000 times with 100 subjects (n = 100) and 30% censoring (c = 0.30)
results <- replicate(1000, study(n = 100, c = 0.30))
# proportion of significant results
mean(results)
```

We would want this power estimate to be higher, say around 0.8 or 0.9. 

To increase power in survival analysis _we need more events_, not just a bigger sample size. Notice by simply assuming a censoring proportion of 0.05, our sample size of 100 now yields about 0.80 power.

```{r}
results <- replicate(1000, study(n = 100, c = 0.05))
# proportion of significant results
mean(results)
```

### How to calculate Kaplan-Meier estimates

Recall the overall Kaplan-Meier estimates of the leukemia data.

```{r}
fit <- survfit(Surv(survt, status) ~ 1, data = d)
summary(fit)
```

Below we show how the first few values in the "survival" column are calculated.

At week 1 (row 1) we have 42 subjects at risk (n.risk) and two subjects who experienced the event (n.event). The estimated probability of survival to week 1 is calculated as (n.risk - n.event)/n.event 

```{r}
(42 - 2)/42
```

At week 2 (row 2) we have 40 subjects at risk and two subjects who experienced the event. The estimated probability of survival to week 2 is calculated as (n.risk - n.event)/n.event _times the estimated probability for week 1 survival_.

```{r}
((42 - 2)/42) * ((40 - 2)/40)
```

At week 3 (row 3) we have 38 subjects at risk and one subject who experienced the event. The estimated probability of survival to week 3 is calculated as (n.risk - n.event)/n.event _times the estimated probabilities for week 1 and 2 survival_.

```{r}
((42 - 2)/42) * ((40 - 2)/40) * ((38 - 1)/38)
```

And so forth. For this reason, the Kaplan-Meier estimator is also called the _product limit estimator_. At each time point, the estimated probability of survival is obtained by _successive multiplication_ of the estimated conditional probabilities. 

### How survfit() calculates covariate-adjusted survival curves

Recall model m2 we fit using the leukemia data and the covariate-adjusted survival curves we created using the model.

```{r}
m2 <- coxph(Surv(survt, status) ~ rx + logwbc, data = d)
```

Below we show the values that were plotted using `summary()` instead of the plots. Column "survival1" shows estimated survival probabilities for "trt" and "survival2" shows estimated survival probabilities for "placebo". How are those survival probabilities calculated?

```{r}
nd <- data.frame(rx = c("trt", "placebo"), 
                 logwbc = median(d$logwbc))
sfit <- survfit(m2, newdata = nd)
summary(sfit, censored = TRUE)
```


To find an estimated survival curve for a set of covariate values, z, we use the following function:

$$S(t|z) = [S_0(t)]^{\text{exp}(z\hat{\beta})}$$ 

where $S_0(t)$ is the _baseline survival function_. We can derive the baseline survival function from the following relationship:

$$S_0(t) = \text{exp}[-H_0(t)]$$ 

where $H_0(t)$ is the _baseline cumulative hazard function_, which is the cumulative sum of the estimated baseline hazards, $h_0(t_j)$. The estimated baseline hazards can be derived using the following function:

$$h_0(t_i) = \frac{d_i}{\sum\limits_{i\in{R_i}}\text{exp}(z_i\hat{\beta})} $$ 

where $d_i$ are the number of subjects who fail at time i, $R_i$ is the number at risk at time i, and $z_i$ are the covariates used to fit the model.

Let's do this "by hand" for the model m2 above. 

First we use the `basehaz()` function to calculate the cumulative hazards, $H_0$.

```{r}
H0 <- basehaz(m2)
```

Next we calculate the baseline survival function:

```{r}
S0 <- exp(-H0)
```

And then we calculate the estimated survival curves for the two covariate values. We need to convert our new data to a matrix that contains only numbers. This means converting "trt" and "placebo" to 0 and 1. Notice we also center the data by subtracting the means of the covariates from the linear predictor: `as.matrix(nd) %*% beta`. (This is what survfit does.) Finally we calculate the adjusted curves by raising the baseline survival to the power of the linear predictor, zB. The `outer()` function allows us to do this in one line. 

```{r}
nd <- matrix(c(0,1,2.8,2.8), ncol = 2)
beta <- coef(m2)
xcenter <- sum(m2$means * beta)
zB <- exp(c(as.matrix(nd) %*% beta) - xcenter)
St <- outer(S0$hazard, zB, "^")
round(St, 4)
```

Compare St to `survfit()` output to verify they match.

```{r}
summary(sfit, censored = TRUE)
```

